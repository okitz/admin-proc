{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# グラフ抽出 (Graph Extraction)\n",
    "\n",
    "収集したテキストデータから手続きの依存関係グラフを抽出する。\n",
    "LLMを使用してテキストを解析し、JSON形式のグラフデータに変換する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rorok/eeic_lectures/3a/superD/admin-proc/.venv/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.12) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import asyncio\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import config\n",
    "\n",
    "# 設定\n",
    "PROMPT_FILE = \"../prompt.md\"\n",
    "\n",
    "# LLM設定\n",
    "# MODEL_NAME = \"gemma3:12b\"\n",
    "MODEL_NAME = \"qwen3-vl:4b\"\n",
    "# BASE_URL = \"http://localhost:11434\"\n",
    "BASE_URL = \"http://192.168.0.59:11434\"\n",
    "\n",
    "GEMINI_MODEL = \"gemini-2.5-pro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionType(str, Enum):\n",
    "    Physical_Go = \"Physical_Go\"\n",
    "    Physical_Mail = \"Physical_Mail\"\n",
    "    Physical_Copy = \"Physical_Copy\"\n",
    "    Physical_Print = \"Physical_Print\"\n",
    "    Physical_Fill = \"Physical_Fill\"\n",
    "    Physical_Attach = \"Physical_Attach\"\n",
    "    External_Acquire = \"External_Acquire\"\n",
    "    Digital_Input = \"Digital_Input\"\n",
    "    Digital_Auth = \"Digital_Auth\"\n",
    "    Digital_Upload = \"Digital_Upload\"\n",
    "    Digital_Capture = \"Digital_Capture\"\n",
    "    Digital_Submit = \"Digital_Submit\"\n",
    "    Wait_Process = \"Wait_Process\"\n",
    "    No_Action = \"No_Action\"\n",
    "\n",
    "class ActionCategory(str, Enum):\n",
    "    Work = \"Work\"\n",
    "    Move = \"Move\"\n",
    "    Wait = \"Wait\"\n",
    "\n",
    "class GraphEdge(BaseModel):\n",
    "    source: str = Field(description=\"The starting point or prerequisite of the action\")\n",
    "    target: str = Field(description=\"The result or next step of the action\")\n",
    "    action: str = Field(description=\"The specific action taken\")\n",
    "    type: ActionType = Field(description=\"The type of cost/action\")\n",
    "    category: ActionCategory = Field(description=\"The subject/category of the action\")\n",
    "\n",
    "class GraphData(BaseModel):\n",
    "    analog: List[GraphEdge] = Field(description=\"Dependency graph for analog application\")\n",
    "    digital: List[GraphEdge] = Field(description=\"Dependency graph for digital application\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "model_local = ChatOllama(\n",
    "    model=MODEL_NAME,\n",
    "    base_url=BASE_URL,\n",
    "    temperature=0.1\n",
    ").with_structured_output(GraphData)\n",
    "\n",
    "model_gemini = ChatGoogleGenerativeAI(\n",
    "    model=GEMINI_MODEL,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ").with_structured_output(\n",
    "    schema=GraphData.model_json_schema(), method=\"json_schema\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chain(prompt_path: str):\n",
    "    with open(prompt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        prompt_text = f.read()\n",
    "    \n",
    "    # Create Prompt Template\n",
    "    # prompt.md contains {input_homepage} and {input_digital}\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "    \n",
    "    # Create Chain\n",
    "    # chain = prompt | model_local\n",
    "    chain = prompt | model_gemini\n",
    "    return chain\n",
    "\n",
    "async def extract_graph(chain, homepage_text, digital_text):\n",
    "    try:\n",
    "        result = await chain.ainvoke({\n",
    "            \"input_homepage\": homepage_text,\n",
    "            \"input_digital\": digital_text\n",
    "        })\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error during extraction: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 福岡市 (40130)...\n",
      "  -> Saved prompt to /home/rorok/eeic_lectures/3a/superD/admin-proc/data/prompts/prompt_40130.md\n",
      "{'analog': [{'source': '開始(自宅)', 'target': '区役所窓口', 'action': '区役所へ移動', 'type': 'Physical_Go', 'category': 'Move'}, {'source': '区役所窓口', 'target': '申請書(紙)', 'action': '窓口で申請書を入手', 'type': 'External_Acquire', 'category': 'Work'}, {'source': '申請書(紙)', 'target': '申請書(記入済)', 'action': '窓口で必要事項を記入', 'type': 'Physical_Fill', 'category': 'Work'}, {'source': '申請書(記入済)', 'target': '申請完了', 'action': 'マイナンバーカード・通帳を提示し提出', 'type': 'Physical_Attach', 'category': 'Work'}, {'source': '申請完了', 'target': '審査完了', 'action': '自治体による審査', 'type': 'Wait_Process', 'category': 'Wait'}, {'source': '審査完了', 'target': '手当受給', 'action': '認定通知書受領・口座振込', 'type': 'Wait_Process', 'category': 'Wait'}, {'source': '開始(自宅)', 'target': '所得証明書', 'action': '情報連携により省略', 'type': 'No_Action', 'category': 'Work'}, {'source': '開始(自宅)', 'target': '健康保険証の写し', 'action': '組合健保のため提出不要', 'type': 'No_Action', 'category': 'Work'}, {'source': '開始(自宅)', 'target': '児童の住民票', 'action': '情報連携により省略', 'type': 'No_Action', 'category': 'Work'}], 'digital': [{'source': '開始(自宅)', 'target': 'マイナポータル', 'action': 'マイナポータルにアクセスしログイン', 'type': 'Digital_Auth', 'category': 'Work'}, {'source': 'マイナポータル', 'target': '申請画面', 'action': '児童手当 認定請求を検索・選択', 'type': 'Digital_Input', 'category': 'Work'}, {'source': '申請画面', 'target': '入力内容確認画面', 'action': '申請者・児童・振込口座等の情報を入力', 'type': 'Digital_Input', 'category': 'Work'}, {'source': '入力内容確認画面', 'target': '電子署名', 'action': '入力内容を確認', 'type': 'Digital_Input', 'category': 'Work'}, {'source': '電子署名', 'target': '申請データ', 'action': 'マイナンバーカードで電子署名', 'type': 'Digital_Auth', 'category': 'Work'}, {'source': '申請データ', 'target': '申請完了', 'action': '申請データを送信', 'type': 'Digital_Submit', 'category': 'Work'}, {'source': '申請完了', 'target': '審査完了', 'action': '自治体による審査', 'type': 'Wait_Process', 'category': 'Wait'}, {'source': '審査完了', 'target': '手当受給', 'action': '認定通知書受領・口座振込', 'type': 'Wait_Process', 'category': 'Wait'}, {'source': '開始(自宅)', 'target': '所得証明書', 'action': '情報連携により省略', 'type': 'No_Action', 'category': 'Work'}, {'source': '開始(自宅)', 'target': '健康保険証の写し', 'action': '組合健保のため提出不要', 'type': 'No_Action', 'category': 'Work'}, {'source': '開始(自宅)', 'target': '児童の住民票', 'action': '情報連携により省略', 'type': 'No_Action', 'category': 'Work'}, {'source': '開始(自宅)', 'target': '通帳・キャッシュカードの写し', 'action': '公金受取口座利用により省略', 'type': 'No_Action', 'category': 'Work'}]}\n",
      "  -> Saved graph to /home/rorok/eeic_lectures/3a/superD/admin-proc/data/processed_graph/40130.json\n"
     ]
    }
   ],
   "source": [
    "chain = create_chain(PROMPT_FILE)\n",
    "\n",
    "# Ensure prompts directory exists\n",
    "PROMPTS_DIR = os.path.join(config.DATA_DIR, \"prompts\")\n",
    "os.makedirs(PROMPTS_DIR, exist_ok=True)\n",
    "\n",
    "for city in config.TARGET_CITIES:\n",
    "    city_id = city[\"id\"]\n",
    "    city_name = city[\"name\"]\n",
    "    print(f\"Processing {city_name} ({city_id})...\")\n",
    "    \n",
    "    # ファイルの検索\n",
    "    homepage_files = glob.glob(os.path.join(config.RAW_TEXT_DIR, f\"{city_id}_homepage*.txt\"))\n",
    "    digital_files = glob.glob(os.path.join(config.RAW_TEXT_DIR, f\"{city_id}_digital*.txt\"))\n",
    "    \n",
    "    if not homepage_files or not digital_files:\n",
    "        print(f\"  -> Missing files for {city_name}. Skipping.\")\n",
    "        continue\n",
    "        \n",
    "    with open(homepage_files[0], \"r\", encoding=\"utf-8\") as f:\n",
    "        homepage_text = f.read()\n",
    "    with open(digital_files[0], \"r\", encoding=\"utf-8\") as f:\n",
    "        digital_text = f.read()\n",
    "    \n",
    "    # Promptの生成と保存\n",
    "    # ChatPromptTemplateからフォーマット済みメッセージを取得\n",
    "    # chain.firstはChatPromptTemplate\n",
    "    prompt_template = chain.first\n",
    "    formatted_prompt_value = prompt_template.invoke({\n",
    "        \"input_homepage\": homepage_text,\n",
    "        \"input_digital\": digital_text\n",
    "    })\n",
    "    # 文字列として取得\n",
    "    formatted_text = formatted_prompt_value.to_string()\n",
    "    \n",
    "    prompt_output_path = os.path.join(PROMPTS_DIR, f\"prompt_{city_id}.md\")\n",
    "    with open(prompt_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(formatted_text)\n",
    "    print(f\"  -> Saved prompt to {prompt_output_path}\")\n",
    "        \n",
    "    # グラフ抽出\n",
    "    graph_data = await extract_graph(chain, homepage_text, digital_text)\n",
    "    \n",
    "    if graph_data:\n",
    "        output_path = os.path.join(config.PROC_GRAPH_DIR, f\"{city_id}.json\")\n",
    "        print(graph_data)\n",
    "        json_data = graph_data\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"  -> Saved graph to {output_path}\")\n",
    "    else:\n",
    "        print(f\"  -> Failed to extract graph for {city_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "admin-proc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
