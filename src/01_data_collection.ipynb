{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## データ収集 (Data Collection)\n",
                "\n",
                "本ノートブックでは、対象自治体の児童手当に関するWebページを収集します。\n",
                "\n",
                "### フロー\n",
                "1. **自動検索 & 候補抽出**: 定義された検索パターンごとに、指定された優先ドメインを持つURLを抽出し、`data/source_urls.json` に保存。\n",
                "2. **手動確認**: `data/source_urls.json` を確認・修正。\n",
                "3. **データ保存**: 修正後の `data/source_urls.json` を読み込み、データをダウンロードして保存。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import time\n",
                "import requests\n",
                "import hashlib\n",
                "from bs4 import BeautifulSoup\n",
                "from googleapiclient.discovery import build\n",
                "import config\n",
                "\n",
                "# 設定\n",
                "DATA_DIR = \"../data/raw_text\"\n",
                "HTML_CACHE_DIR = \"../data/raw_html\"\n",
                "JSON_PATH = \"../data/source_urls.json\"\n",
                "os.makedirs(DATA_DIR, exist_ok=True)\n",
                "os.makedirs(HTML_CACHE_DIR, exist_ok=True)\n",
                "\n",
                "ENABLE_GOOGLE_SEARCH = False\n",
                "GOOGLE_API_KEY = \"AIzaSyCfGA5ChqTMDHFrja8CdJM9-GS409Pq8yk\"\n",
                "CUSTOM_SEARCH_ENGINE_ID = \"3420ec94d15d34b85\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_html_content(url):\n",
                "    \"\"\"\n",
                "    URLからHTMLを取得する。キャッシュがあればそれを使う。\n",
                "    \"\"\"\n",
                "    url_hash = hashlib.md5(url.encode('utf-8')).hexdigest()\n",
                "    cache_path = os.path.join(HTML_CACHE_DIR, f\"{url_hash}.html\")\n",
                "    \n",
                "    if os.path.exists(cache_path):\n",
                "        print(f\"  -> Loading from cache: {cache_path}\")\n",
                "        with open(cache_path, 'r', encoding='utf-8') as f:\n",
                "            return f.read()\n",
                "            \n",
                "    print(f\"Fetching: {url}\")\n",
                "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
                "    res = requests.get(url, headers=headers, timeout=10)\n",
                "    res.raise_for_status()\n",
                "    res.encoding = res.apparent_encoding\n",
                "    html_content = res.text\n",
                "    \n",
                "    with open(cache_path, 'w', encoding='utf-8') as f:\n",
                "        f.write(html_content)\n",
                "    \n",
                "    return html_content\n",
                "\n",
                "def clean_html(soup):\n",
                "    \"\"\"\n",
                "    不要な要素(header, nav, footer, script, style)を削除する\n",
                "    \"\"\"\n",
                "    for element in soup(['header', 'nav', 'footer', 'script', 'style']):\n",
                "        element.decompose()\n",
                "    return soup\n",
                "\n",
                "def convert_tables_to_markdown(soup):\n",
                "    \"\"\"\n",
                "    HTMLのtable要素をMarkdown形式のテキストに変換して置換する\n",
                "    \"\"\"\n",
                "    for table in soup.find_all('table'):\n",
                "        rows = table.find_all('tr')\n",
                "        if not rows:\n",
                "            continue\n",
                "            \n",
                "        markdown_table = []\n",
                "        \n",
                "        # Header\n",
                "        headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]\n",
                "        if headers:\n",
                "            markdown_table.append('| ' + ' | '.join(headers) + ' |')\n",
                "            markdown_table.append('| ' + ' | '.join(['---'] * len(headers)) + ' |')\n",
                "        \n",
                "        # Body\n",
                "        for row in rows[1:]:\n",
                "            cols = [td.get_text(strip=True) for td in row.find_all(['td', 'th'])]\n",
                "            if cols:\n",
                "                # Adjust column count if necessary (simple handling)\n",
                "                if len(cols) < len(headers):\n",
                "                    cols += [''] * (len(headers) - len(cols))\n",
                "                elif len(cols) > len(headers) and headers:\n",
                "                     # If header is missing but body has more cols, we might need to adjust header or just ignore mismatch for now\n",
                "                     pass\n",
                "                markdown_table.append('| ' + ' | '.join(cols) + ' |')\n",
                "        \n",
                "        # Replace table with markdown\n",
                "        new_tag = soup.new_tag('div')\n",
                "        new_tag.string = '\\n' + '\\n'.join(markdown_table) + '\\n'\n",
                "        table.replace_with(new_tag)\n",
                "    \n",
                "    return soup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 検索パターンと優先ドメイン\n",
                "SEARCH_CONFIGS = [\n",
                "    {\n",
                "        \"pattern_suffix\": \"児童手当 認定請求 site:app.oss.myna.go.jp\",\n",
                "        \"trusted_domains\": [\"app.oss.myna.go.jp\"],\n",
                "        \"file_suffix\": \"digital\"\n",
                "    },\n",
                "    {\n",
                "        \"pattern_suffix\": \"児童手当 認定請求\",\n",
                "        \"trusted_domains\": [\".lg.jp\", \"city.\"], \n",
                "        \"file_suffix\": \"homepage\"\n",
                "    },\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Google Search API is disabled\n"
                    ]
                }
            ],
            "source": [
                "def get_google_search_results(query, trusted_domains, num_results=10):\n",
                "    \"\"\"\n",
                "    Google Custom Search APIを使用して検索し、\n",
                "    優先ドメインを含むURLがあればそれを、なければ最上位を返す。\n",
                "    \"\"\"\n",
                "    if not ENABLE_GOOGLE_SEARCH:\n",
                "        print(\"Google Search API is disabled\")\n",
                "        return\n",
                "    print(f\"Searching API: {query} ...\")\n",
                "    \n",
                "    try:\n",
                "        service = build(\"customsearch\", \"v1\", developerKey=GOOGLE_API_KEY)\n",
                "        \n",
                "        # API Call\n",
                "        res = service.cse().list(\n",
                "            q=query,\n",
                "            cx=CUSTOM_SEARCH_ENGINE_ID,\n",
                "            lr='lang_ja',\n",
                "            num=num_results\n",
                "        ).execute()\n",
                "        \n",
                "        items = res.get(\"items\", [])\n",
                "        if not items:\n",
                "            print(\"  -> No results found.\")\n",
                "            return None\n",
                "            \n",
                "        candidates = [item['link'] for item in items]\n",
                "        \n",
                "        # フィルタリングロジック\n",
                "        # 1. trusted_domains に一致するものがあれば、その一番上を返す\n",
                "        for url in candidates:\n",
                "            for domain in trusted_domains:\n",
                "                if domain in url:\n",
                "                    return url\n",
                "        \n",
                "        # 2. なければトップを返す\n",
                "        if candidates:\n",
                "            return candidates[0]\n",
                "            \n",
                "    except Exception as e:\n",
                "        print(f\"API Error: {e}\")\n",
                "        return None\n",
                "    \n",
                "    return None\n",
                "\n",
                "def search_city_urls():\n",
                "    extracted_data = []\n",
                "    for city in config.TARGET_CITIES:\n",
                "        city_name = city[\"name\"]\n",
                "        \n",
                "        for config in SEARCH_CONFIGS:\n",
                "            pattern = config[\"pattern_suffix\"]\n",
                "            domains = config[\"trusted_domains\"]\n",
                "            \n",
                "            full_query = f\"{city_name} {pattern}\"\n",
                "            best_url = get_google_search_results(full_query, domains)\n",
                "            \n",
                "            entry = {\n",
                "                \"city_id\": city[\"id\"],\n",
                "                \"city_name\": city_name,\n",
                "                \"search_pattern\": pattern,\n",
                "                \"query\": full_query,\n",
                "                \"url\": best_url if best_url else \"\",\n",
                "                \"file_suffix\": config[\"file_suffix\"]\n",
                "            }\n",
                "            extracted_data.append(entry)\n",
                "            print(f\"  -> Selected: {best_url}\")\n",
                "            time.sleep(1)\n",
                "    return extracted_data\n",
                "\n",
                "\n",
                "if ENABLE_GOOGLE_SEARCH:\n",
                "    print(\"検索を開始します...\")\n",
                "    search_city_urls()\n",
                "    with open(JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
                "        json.dump(extracted_data, f, indent=2, ensure_ascii=False)\n",
                "    print(f\"\\n検索完了。結果を {JSON_PATH} に保存しました。\")\n",
                "else:\n",
                "    print(\"Google Search API is disabled\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### スクレイピング\n",
                "\n",
                "作成された `source_urls.json` を開いて確認し、必要に応じてURLを修正\n",
                "確認が終わったら、データをダウンロード"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "6 件のURLを処理します...\n",
                        "  -> Loading from cache: ../data/raw_html/a005f9d3f508ffbd04e57bc895c4a653.html\n",
                        "  -> Saved to 13105_digital.txt\n",
                        "  -> Loading from cache: ../data/raw_html/a174e1bd681202caecdea4aa681657f5.html\n",
                        "  -> Saved to 13105_general.txt\n",
                        "  -> Loading from cache: ../data/raw_html/334f1ac7f7ede2c5f8ef7ef0f91dc5fa.html\n",
                        "  -> Saved to 13106_digital.txt\n",
                        "  -> Loading from cache: ../data/raw_html/e6ac298379a32f65611709afd65a98de.html\n",
                        "  -> Saved to 13106_general.txt\n",
                        "  -> Loading from cache: ../data/raw_html/12923589610933bd656fba75259a02f3.html\n",
                        "  -> Saved to 47201_digital.txt\n",
                        "  -> Loading from cache: ../data/raw_html/565a9af2a7bf989f8464eb050412ce28.html\n",
                        "  -> Saved to 47201_general.txt\n"
                    ]
                }
            ],
            "source": [
                "# JSONの読み込みとデータ取得\n",
                "if not os.path.exists(JSON_PATH):\n",
                "    print(f\"{JSON_PATH} が見つかりません。\")\n",
                "else:\n",
                "    with open(JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
                "        source_list = json.load(f)\n",
                "    \n",
                "    print(f\"{len(source_list)} 件のURLを処理します...\")\n",
                "    \n",
                "    for item in source_list:\n",
                "        url = item.get(\"url\")\n",
                "        city_id = item.get(\"city_id\")\n",
                "        pattern = item.get(\"search_pattern\")\n",
                "        file_suffix = item.get(\"file_suffix\", \"misc\")\n",
                "        \n",
                "        if not url:\n",
                "            print(f\"Skipping empty URL for {item['city_name']} - {pattern}\")\n",
                "            continue\n",
                "            \n",
                "        try:\n",
                "            # 1. HTML取得\n",
                "            html_content = get_html_content(url)\n",
                "            \n",
                "            # 2. 解析 & クリーニング\n",
                "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
                "            soup = clean_html(soup)\n",
                "            \n",
                "            # 3. テーブル変換\n",
                "            soup = convert_tables_to_markdown(soup)\n",
                "            \n",
                "            # 4. テキスト抽出\n",
                "            text = soup.get_text(separator=\"\\n\", strip=True)\n",
                "            \n",
                "            # ファイル名生成: city_id + configで指定されたsuffix\n",
                "            base_filename = f\"{city_id}_{file_suffix}\"\n",
                "            filename = f\"{base_filename}.txt\"\n",
                "            \n",
                "            filepath = os.path.join(DATA_DIR, filename)\n",
                "            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
                "                f.write(f\"Source URL: {url}\\n\")\n",
                "                f.write(f\"Search Pattern: {pattern}\\n\")\n",
                "                f.write(\"\\n\")\n",
                "                f.write(text)\n",
                "            \n",
                "            print(f\"  -> Saved to {filename}\")\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"  -> Error fetching {url}: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "admin-proc",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
